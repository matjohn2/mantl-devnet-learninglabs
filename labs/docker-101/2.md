# Who made hello-world?

So how did the author of hello-world make a docker image?

As hello-world came from the public repository, we can go and inspect it to work that out.

Going to [https://hub.docker.com/_/hello-world/]() provides a website with details of the hello-world docker image.

This doesn't give you much more than documentation, but theres a very important link on this page to the *Dockerfile*

![docker hub website](/posts/files/docker-101/assets/images/dockerhub1.png)

![Dockerfile](/posts/files/docker-101/assets/images/dockerfile1.png)

# The Dockerfile

The Dockerfile is how everyone builds docker images, from something as simple as hello-world, to full applications.

It's a simple text file in which you describe what you want in your container image, docker then takes this and builds you an image based on what you have defined.

To do this, the author would run `docker build .`

This tells docker to process the docker file in the current directory.

Once built, the author of hello-world had also 'pushed' that image to the public docker hub, so that it was consumable by everyone.

Usually, a Dockerfile is stored with your application code in source control.

![hello-world codebase](/posts/files/docker-101/assets/images/hello-codebase.png)

# Hello-world Dockerfile

This is the hello-world Dockerfile:

```
FROM scratch
COPY hello /
CMD ["/hello"]
```

Simple right? here's what it tells `docker build .` to do:

    FROM scratch
    * We can build on existing docker images if we wish. ’scratch’ means start with a completely empty container.

    COPY hello /
    * Copy the local file ‘hello’ into the container image. (This is why we keep Dockerfiles in our application source directory, so we can refer to the pieces of our app we want pulled into the container.)

    CMD ["/hello"]
    * The command to run inside the image whenever someone `docker run`'s your image.

# App dependancies

Putting your apps in a container isolates them, so the app still needs all it's dependancies IN the container image.

in the hello-world example, the compiled binary ‘hello’ was the only thing in the container, it works because it needed no dependencies. Putting it in a container isolated it from the rest of the system and made it easily shareable with other developers who can simply run `docker run hello-world`.

But what if we wanted to run something more complex like NGINX (a webserver) as part of our containerized application? There will be a lot more dependancies, system libraries needed etc.

ldd shows us linked libraries (dependancies) needed by a C application; we can see NGINX has dependancies:

![NGINX dependancies](/posts/files/docker-101/assets/images/ldd1.png)

In the Sever / VM model, these dependencies would be installed as extra packages via your package management tool (APT, YUM, etc) however we have a blank container, theres no apt, yum tools..

## Full OS userspace containers

A container can have a whole Linux userspace within it to build on (such as Ubuntu) in order to make complex software installs easier within containers.

The downside being the container image will end up being a lot bigger (100's of MB), however it means your Dockerfile can contain `apt-get instal` commands (because those tools are already IN the container).

```
docker search ubuntu
```

We see an existing ubuntu container image on the public hub.

Run the container now.

```
docker run -ti ubuntu
```

This run's an ubuntu container for us, notice it drops us into the ubuntu BASH shell within the container.

![Ubuntu shell](/posts/files/docker-101/assets/images/ubuntu1.png)

(This is what `-ti` is for, it tells docker we want to interact on the terminal with the container.)

# Testing Isolation.

In our container, if we create a file in root (/), notice that when we exit the container, the file doesnt exist on the VM outside the container.

The container has no idea theres anything outside of it.

```
touch /hello
ls /

#exit the container
exit

#run an LS on the VM
ls /
```

Notice the hello file was not there on the last `ls`, it was only within the container.

Also, in this instance, typing `exit` in the container has killed the container. We can see which containers are running on our system with `docker ps`

Notice there are no containers running.

If we run `docker run -ti ubuntu` again, this will be a BRAND NEW instance of the ubuntu container, the `/hello` file won't be there, as it wasnt baked into the ubuntu docker image.

### Changes to a running docker container dont survive once the container is killed.

If you want something in every instance of your docker container, you bake it in by updating the Dockerfile and rebuilding your container image.

# The ubuntu Dockerfile

Lets look at the Dockerfile which built the ubuntu image we just ran...

You can find it once again at [https://hub.docker.com/_/ubuntu/]()

Click though to the Dockerfile just like we did with hello-world.

Notice theres a lot more in the DockerFile, but the concepts are exactly the same:

```
FROM scratch
ADD ubuntu-xenial-core-cloudimg-amd64-root.tar.gz /

CMD ["/bin/bash"]
```

The dockerfile gets a whole ubuntu userspace (all the tools, binaries, libraries) from a compressed .tar file and extracts it to the root of the image.

Why did the ubuntu container give us a BASH shell when we ran it? Because thats the CMD specified in the Dockerfile!

Again the Dockerfile is in the same source code repository as the needed files (such as the .tar.gz archive)

![Ubuntu Repo for Docker Build](/posts/files/docker-101/assets/images/ubunturepo1.png)

I think we know enough now to build our OWN docker container with a dockerfile... Click next to get going!
